{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.12+"
    },
    "colab": {
      "name": "hmm.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "10scrMKB-iCu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import copy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_2XZ9Zk-iC1",
        "colab_type": "text"
      },
      "source": [
        "# Demonstration of a hidden Markov Model\n",
        "by Jiaxin Shi, ishijiaxin@126.com (from https://github.com/thjashin/hmm)\n",
        "\n",
        "modified by Lior Pachter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4jGsiumALQg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b3b73f98-ba3a-4f25-dee4-38e1ea668f3e"
      },
      "source": [
        "!date"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Jan 14 17:09:49 UTC 2020\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6F-K1xS-iC3",
        "colab_type": "text"
      },
      "source": [
        "# 0. Implementation\n",
        "\n",
        "This notebook implements the following hidden Markov model (HMM) algorithms:\n",
        "- Forward-Backward (sum-product)\n",
        "- Viterbi (max-product)\n",
        "- Baum-Welch (expectation-maximization algorithm)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOxZVQN1-iC4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!/usr/bin/env python\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "import numpy as np\n",
        "import copy\n",
        "\n",
        "\n",
        "class HMM:\n",
        "    def __init__(self, states, transition, emission, init):\n",
        "        self.state_names = copy.copy(states)\n",
        "        self.n_states = len(states)\n",
        "        self.A = transition.copy()\n",
        "        self.B = emission.copy()\n",
        "        self.n_emissions = self.B.shape[1]\n",
        "        self.init = init\n",
        "\n",
        "    def generate(self, length):\n",
        "        state = self.init\n",
        "        states = []\n",
        "        ret = []\n",
        "        for i in xrange(1, length + 1):\n",
        "            state = np.random.choice(range(self.n_states), p=self.A[state])\n",
        "            states.append(state)\n",
        "            ret.append(\n",
        "                np.random.choice(range(self.n_emissions), p=self.B[state]))\n",
        "        print 'Generating by states:', ''.join(self.state_names[i] for i in states)\n",
        "        ret = ''.join([str(i) for i in ret])\n",
        "        return ret\n",
        "\n",
        "    def _forward(self, seq_arr):\n",
        "        T = len(seq_arr)\n",
        "        alpha = np.zeros((T + 1, self.n_states))\n",
        "        alpha[0, self.init] = 1\n",
        "        log_px = 0.\n",
        "        for t in xrange(1, T + 1):\n",
        "            alpha[t] = self.B[:, seq_arr[t - 1]] * \\\n",
        "                       np.dot(alpha[t - 1], self.A)\n",
        "            pt = alpha[t].sum()\n",
        "            alpha[t] /= pt\n",
        "            log_px += np.log(pt)\n",
        "        return alpha, log_px\n",
        "\n",
        "    def _backward(self, seq_arr):\n",
        "        T = len(seq_arr)\n",
        "        beta = np.zeros((T + 1, self.n_states))\n",
        "        beta[T, :] = 1\n",
        "        log_px = 0.\n",
        "        for t in xrange(T, 0, -1):\n",
        "            beta[t - 1] = np.dot(self.A, beta[t] * self.B[:, seq_arr[t - 1]])\n",
        "            pt = beta[t - 1].sum()\n",
        "            beta[t - 1] /= pt\n",
        "            log_px += np.log(pt)\n",
        "        log_px += np.log(beta[0, self.init])\n",
        "        return beta, log_px\n",
        "\n",
        "    def viterbi(self, seq):\n",
        "        # := max-product\n",
        "        seq_arr = np.array([int(i) for i in seq])\n",
        "        T = len(seq_arr)\n",
        "        T1 = np.zeros((self.n_states, T + 1))\n",
        "        T1[self.init, 0] = 1\n",
        "        T2 = np.zeros((self.n_states, T + 1), dtype='int')\n",
        "        states = np.zeros(T + 1, dtype='int')\n",
        "        for t in xrange(1, T + 1):\n",
        "            for j in xrange(self.n_states):\n",
        "                T1[j, t] = np.max(T1[:, t - 1] * self.A[:, j])\n",
        "                T1[j, t] *= self.B[j, seq_arr[t - 1]]\n",
        "                T2[j, t] = np.argmax(T1[:, t - 1] * self.A[:, j])\n",
        "        states[T] = np.argmax(T1[:, T])\n",
        "        for t in xrange(T, 1, -1):\n",
        "            states[t - 1] = T2[states[t], t - 1]\n",
        "        return ''.join([self.state_names[s] for s in states[1:]])\n",
        "\n",
        "    def baum_welch(self, seq):\n",
        "        # := EM\n",
        "        seq_arr = np.array([int(i) for i in seq])\n",
        "        T = len(seq_arr)\n",
        "        kesi = np.zeros((T + 1, self.n_states, self.n_states))\n",
        "        log_px = None\n",
        "        iter = 0\n",
        "        while True:\n",
        "            iter += 1\n",
        "            alpha, alpha_log_px = self._forward(seq_arr)\n",
        "            print \"Iter %d\" % iter, \"log p(x): %s\" % alpha_log_px\n",
        "            if log_px and (np.abs(\n",
        "                    log_px - alpha_log_px) < np.abs(1e-6 * log_px)):\n",
        "                print \"Converged.\"\n",
        "                break\n",
        "            beta, beta_log_px = self._backward(seq_arr)\n",
        "            try:\n",
        "                assert np.abs(\n",
        "                    alpha_log_px - beta_log_px) < np.abs(1e-6 * alpha_log_px)\n",
        "            except AssertionError as e:\n",
        "                print \"alpha_log_px:\", alpha_log_px\n",
        "                print \"beta_log_px:\", beta_log_px\n",
        "                raise e\n",
        "            log_px = alpha_log_px\n",
        "            gamma = alpha * beta\n",
        "            gamma /= np.sum(gamma, axis=1, keepdims=True)\n",
        "            for t in xrange(1, T):\n",
        "                kesi[t] = np.outer(\n",
        "                    alpha[t],\n",
        "                    beta[t + 1] * self.B[:, seq_arr[t + 1 - 1]]) * self.A\n",
        "            kesi[1:T] = kesi[1:T] / kesi[1:T].sum(axis=(1, 2), keepdims=True)\n",
        "            self.A = kesi[1:T].sum(axis=0) / \\\n",
        "                     gamma[1:T].sum(axis=0)[:, np.newaxis]\n",
        "            assert np.all(np.abs(1. - self.A.sum(axis=1)) < 1e-6)\n",
        "            obs = np.zeros((T + 1, self.n_emissions))\n",
        "            obs[range(1, T + 1), seq_arr] = 1\n",
        "            self.B = np.dot(gamma[1:].T, obs[1:]) / \\\n",
        "                     gamma[1:].sum(axis=0)[:, np.newaxis]\n",
        "        print \"Estimate A:\"\n",
        "        print np.array_str(self.A, precision=3)\n",
        "        print \"Estimate B:\"\n",
        "        print np.array_str(self.B, precision=3)\n",
        "        return log_px, self.A, self.B\n",
        "\n",
        "    def gibbs(self, seq, steps=1, burn_in=0, max_iters=None):\n",
        "        seq_arr = np.array([int(i) for i in seq])\n",
        "        T = len(seq_arr)\n",
        "        states = np.zeros(T + 1, dtype='int')\n",
        "        iter = 0\n",
        "        log_px = None\n",
        "        states[0] = self.init\n",
        "        while True:\n",
        "            iter += 1\n",
        "            alpha, alpha_log_px = self._forward(seq_arr)\n",
        "            print \"Iter %d\" % iter, \"log p(x): %s\" % alpha_log_px\n",
        "            if log_px and (np.abs(\n",
        "                    log_px - alpha_log_px) < np.abs(1e-6 * log_px)):\n",
        "                print \"Converged.\"\n",
        "                break\n",
        "            log_px = alpha_log_px\n",
        "            if max_iters and (iter >= max_iters):\n",
        "                break\n",
        "            A = np.zeros_like(self.A)\n",
        "            B = np.zeros_like(self.B)\n",
        "            for t in xrange(1, T + 1):\n",
        "                states[t] = np.random.choice(range(3))\n",
        "            for step in xrange(steps):\n",
        "                for t in xrange(1, T + 1):\n",
        "                    p_state_t = self.B[:, seq_arr[t - 1]] * \\\n",
        "                                self.A[states[t - 1]]\n",
        "                    if t < T:\n",
        "                        p_state_t *= self.A[:, states[t + 1]]\n",
        "                    p_state_t /= p_state_t.sum()\n",
        "                    states[t] = np.random.choice(range(3), p=p_state_t)\n",
        "                if step >= burn_in:\n",
        "                    for t in xrange(1, T + 1):\n",
        "                        if t < T:\n",
        "                            A[states[t], states[t + 1]] += 1\n",
        "                        B[states[t], seq_arr[t - 1]] += 1\n",
        "            A = np.maximum(1., A)\n",
        "            B = np.maximum(1., B)\n",
        "            self.A = A / A.sum(axis=1, keepdims=True)\n",
        "            self.B = B / B.sum(axis=1, keepdims=True)\n",
        "        print \"Estimate A:\"\n",
        "        print np.array_str(self.A, precision=3)\n",
        "        print \"Estimate B:\"\n",
        "        print np.array_str(self.B, precision=3)\n",
        "        return log_px"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "2dLzotUr-iDA",
        "colab_type": "text"
      },
      "source": [
        "## 1.1 Hidden state inference \n",
        "This code generates sequences of lengths 100, 1000 and 10000 (see comment in code) on an alphabet of size 3 from 3 hidden states, and then the Viterbi algorithm is used to infer the most likely hidden states that generated the sequence(s)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "RLFrOAhC-iDC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "a17282a3-dfd5-4b45-ef3c-b8f034b50e17"
      },
      "source": [
        "np.random.seed(1236)\n",
        "states = ['A', 'B', 'C']\n",
        "\n",
        "print \"1.1 Generation\\n\"\n",
        "transition = np.array([\n",
        "    [0.8, 0.2, 0.0],\n",
        "    [0.1, 0.7, 0.2],\n",
        "    [0.1, 0.0, 0.9]\n",
        "])\n",
        "emission = np.array([\n",
        "    [0.9, 0.1],\n",
        "    [0.5, 0.5],\n",
        "    [0.1, 0.9]\n",
        "])\n",
        "init = 0\n",
        "hmm = HMM(states, transition, emission, init)\n",
        "seqs = []\n",
        "for seq_len in [100, 1000, 10000]:\n",
        "    seq = hmm.generate(seq_len)\n",
        "    seqs.append(seq)\n",
        "    print \"Inferred optimal state series:\"\n",
        "    print hmm.viterbi(seq)\n",
        "    # NOTE: To run chains with various length, REMOVE this break\n",
        "    break\n"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.1 Generation\n",
            "\n",
            "Generating by states: AAAAAABBBBCCCCCCCCCCCCCCAAAAAAAABBBBBBBCCAABBBCCCCCCCCCCAABBBBBCCCCCCCCCCCCCCCCABBBABBAAAAAAAAAAAAAA\n",
            "Inferred optimal state series:\n",
            "AAAAAABBBBBCCCCCCCCCCCCCCCAAAAAAAAAABBBBBBBBBBBCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCAAAAAAAAAAAAAAAAAA\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZJUYU5FBMaC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0QaJ3ZB-iDG",
        "colab_type": "text"
      },
      "source": [
        "## 1.2 Baum Welch\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqFBzhOB-iDH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "195acf9b-701c-4000-ff33-c2b30ddc1467"
      },
      "source": [
        "print \"\\n1.2 Baum Welch\"\n",
        "for seq in seqs:\n",
        "    print \"\\nSequence length:\", len(seq)\n",
        "    As = []\n",
        "    Bs = []\n",
        "    for run in xrange(10):\n",
        "        print \"Run\", run\n",
        "        transition2 = np.random.random((3, 3))\n",
        "        transition2 /= transition2.sum(axis=1, keepdims=True)\n",
        "        emission2 = np.random.random((3, 2))\n",
        "        emission2 /= emission2.sum(axis=1, keepdims=True)\n",
        "        print \"Init transition:\"\n",
        "        print transition2\n",
        "        print \"Init emission:\"\n",
        "        print emission2\n",
        "        hmm2 = HMM(states, transition2, emission2, init)\n",
        "        log_px, A, B = hmm2.baum_welch(seq)\n",
        "        As.append(A)\n",
        "        Bs.append(B)\n",
        "        print \"Final log p(x):\", log_px\n",
        "        break\n",
        "   "
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "1.2 Baum Welch\n",
            "\n",
            "Sequence length: 100\n",
            "Run 0\n",
            "Init transition:\n",
            "[[0.41141391 0.43933319 0.1492529 ]\n",
            " [0.64224114 0.283969   0.07378985]\n",
            " [0.21495536 0.31890642 0.46613823]]\n",
            "Init emission:\n",
            "[[0.63570068 0.36429932]\n",
            " [0.09348741 0.90651259]\n",
            " [0.32872192 0.67127808]]\n",
            "Iter 1 log p(x): -72.19318633406532\n",
            "Iter 2 log p(x): -69.69171314424933\n",
            "Iter 3 log p(x): -68.77981999052288\n",
            "Iter 4 log p(x): -67.66782656008486\n",
            "Iter 5 log p(x): -65.82203787983642\n",
            "Iter 6 log p(x): -62.79602914246915\n",
            "Iter 7 log p(x): -59.44891713760399\n",
            "Iter 8 log p(x): -57.56877412931889\n",
            "Iter 9 log p(x): -56.90826500507927\n",
            "Iter 10 log p(x): -56.64218616802919\n",
            "Iter 11 log p(x): -56.50497639816005\n",
            "Iter 12 log p(x): -56.4228378284552\n",
            "Iter 13 log p(x): -56.36503427752464\n",
            "Iter 14 log p(x): -56.3170377131918\n",
            "Iter 15 log p(x): -56.27174756008821\n",
            "Iter 16 log p(x): -56.225296064295726\n",
            "Iter 17 log p(x): -56.17512103660135\n",
            "Iter 18 log p(x): -56.119191411817994\n",
            "Iter 19 log p(x): -56.0558369205126\n",
            "Iter 20 log p(x): -55.9839597360092\n",
            "Iter 21 log p(x): -55.90350714741953\n",
            "Iter 22 log p(x): -55.81598971428851\n",
            "Iter 23 log p(x): -55.72464390115457\n",
            "Iter 24 log p(x): -55.63385285494315\n",
            "Iter 25 log p(x): -55.54792030566644\n",
            "Iter 26 log p(x): -55.46991306035695\n",
            "Iter 27 log p(x): -55.40126180327593\n",
            "Iter 28 log p(x): -55.34210330391463\n",
            "Iter 29 log p(x): -55.29185013346853\n",
            "Iter 30 log p(x): -55.24960796690329\n",
            "Iter 31 log p(x): -55.21438917957923\n",
            "Iter 32 log p(x): -55.18521104739328\n",
            "Iter 33 log p(x): -55.161147439393915\n",
            "Iter 34 log p(x): -55.14135932395154\n",
            "Iter 35 log p(x): -55.12511014018397\n",
            "Iter 36 log p(x): -55.11176907444988\n",
            "Iter 37 log p(x): -55.100805943296095\n",
            "Iter 38 log p(x): -55.091781318710105\n",
            "Iter 39 log p(x): -55.08433467312868\n",
            "Iter 40 log p(x): -55.078172329822166\n",
            "Iter 41 log p(x): -55.07305621787379\n",
            "Iter 42 log p(x): -55.06879390359844\n",
            "Iter 43 log p(x): -55.06523004846313\n",
            "Iter 44 log p(x): -55.06223926136272\n",
            "Iter 45 log p(x): -55.05972021956524\n",
            "Iter 46 log p(x): -55.05759089334374\n",
            "Iter 47 log p(x): -55.05578470185311\n",
            "Iter 48 log p(x): -55.054247437818276\n",
            "Iter 49 log p(x): -55.05293481698499\n",
            "Iter 50 log p(x): -55.051810529528254\n",
            "Iter 51 log p(x): -55.05084469153741\n",
            "Iter 52 log p(x): -55.05001261368938\n",
            "Iter 53 log p(x): -55.04929382060047\n",
            "Iter 54 log p(x): -55.04867126801324\n",
            "Iter 55 log p(x): -55.04813071610765\n",
            "Iter 56 log p(x): -55.04766022615077\n",
            "Iter 57 log p(x): -55.04724975477033\n",
            "Iter 58 log p(x): -55.04689082569584\n",
            "Iter 59 log p(x): -55.04657626316001\n",
            "Iter 60 log p(x): -55.04629997454166\n",
            "Iter 61 log p(x): -55.04605677247586\n",
            "Iter 62 log p(x): -55.045842228712935\n",
            "Iter 63 log p(x): -55.045652553615064\n",
            "Iter 64 log p(x): -55.04548449643302\n",
            "Iter 65 log p(x): -55.04533526249092\n",
            "Iter 66 log p(x): -55.04520244417731\n",
            "Iter 67 log p(x): -55.045083963253695\n",
            "Iter 68 log p(x): -55.04497802247103\n",
            "Iter 69 log p(x): -55.0448830648691\n",
            "Iter 70 log p(x): -55.04479773943967\n",
            "Iter 71 log p(x): -55.044720872075516\n",
            "Iter 72 log p(x): -55.044651440928945\n",
            "Iter 73 log p(x): -55.044588555452755\n",
            "Iter 74 log p(x): -55.04453143853401\n",
            "Iter 75 log p(x): -55.044479411225126\n",
            "Converged.\n",
            "Estimate A:\n",
            "[[9.222e-01 7.773e-02 5.672e-05]\n",
            " [1.400e-01 5.146e-01 3.454e-01]\n",
            " [1.348e-02 1.557e-01 8.308e-01]]\n",
            "Estimate B:\n",
            "[[8.004e-01 1.996e-01]\n",
            " [6.336e-01 3.664e-01]\n",
            " [5.178e-06 1.000e+00]]\n",
            "Final log p(x): -55.04453143853401\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRihXEeCANSa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f3e0a1ee-23e4-4cec-ab3a-aede8ce3df38"
      },
      "source": [
        "!date"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Jan 14 17:09:51 UTC 2020\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}